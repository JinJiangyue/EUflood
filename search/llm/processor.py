"""LLM å¤„ç†æ¨¡å— - 4 ä¸ªæ­¥éª¤çš„æ™ºèƒ½å¤„ç†ã€‚"""

from __future__ import annotations

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..config.settings import Settings, settings
from ..orchestrator.workflow import EventContext
from .client import LLMClient, create_llm_client
from .prompts import (
    build_extraction_prompt,
    build_report_prompt,
    build_validation_prompt,
)

logger = logging.getLogger(__name__)


class LLMProcessor:
    """LLM å¤„ç†å™¨ - 4 ä¸ªæ­¥éª¤çš„æ™ºèƒ½å¤„ç†ã€‚"""

    def __init__(self, config: Settings | None = None):
        self.config = config or settings
        self.client: LLMClient | None = None

    def _get_client(self) -> LLMClient:
        """è·å–æˆ–åˆ›å»º LLM å®¢æˆ·ç«¯ã€‚"""
        if self.client is None:
            try:
                self.client = create_llm_client(self.config)
                logger.info("LLM å®¢æˆ·ç«¯å·²åˆ›å»º: %s", self.config.LLM_PROVIDER)
            except Exception as e:
                logger.error("åˆ›å»º LLM å®¢æˆ·ç«¯å¤±è´¥: %s", e)
                raise
        return self.client

    def process(self, context: EventContext) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„ LLM å¤„ç†æµç¨‹ï¼ˆ4 ä¸ªæ­¥éª¤ï¼‰ã€‚"""
        try:
            # å‡†å¤‡äº‹ä»¶ä¿¡æ¯
            event_info = self._prepare_event_info(context)

            # æ­¥éª¤ 1: äº‹ä»¶éªŒè¯å’Œå†²çªè§£å†³
            logger.info("æ­¥éª¤ 1: äº‹ä»¶éªŒè¯å’Œå†²çªè§£å†³...")
            validation_result = self._step1_validation(context, event_info)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³ç»“æœ
            relevant_items = validation_result.get("relevant_items", [])
            if not relevant_items or len(relevant_items) == 0:
                logger.warning("âš ï¸  éªŒè¯å®Œæˆï¼š0 æ¡ç›¸å…³ç»“æœï¼Œè·³è¿‡åç»­LLMå¤„ç†ä»¥èŠ‚çœæˆæœ¬")
                logger.warning("å¯èƒ½çš„åŸå› ï¼š")
                logger.warning("   1. é¢„è¿‡æ»¤åçš„ç»“æœéƒ½ä¸ç›¸å…³")
                logger.warning("   2. LLMåˆ¤æ–­æ‰€æœ‰ç»“æœéƒ½ä¸å±äºè¯¥äº‹ä»¶")
                logger.warning("   3. äº‹ä»¶ä¿¡æ¯ä¸æœç´¢ç»“æœä¸åŒ¹é…")
                
                # è¿”å›æœ€å°ç»“æœï¼Œä¸è¿›è¡Œåç»­å¤„ç†
                return {
                    "validation": validation_result,
                    "extraction": {
                        "timeline": [],
                        "impact": {},
                    },
                    "media": {
                        "selected_items": [],
                        "rejected_items": [],
                    },
                    "report": self._generate_minimal_report_no_relevant(context, event_info),
                }

            # æ­¥éª¤ 2: æ—¶é—´çº¿å’Œå½±å“æå–
            logger.info("æ­¥éª¤ 2: æ—¶é—´çº¿å’Œå½±å“æå–...")
            extraction_result = self._step2_extraction(
                context, event_info, validation_result
            )

            # æ­¥éª¤ 3: ä»éªŒè¯ç»“æœä¸­æå–å¤šåª’ä½“ï¼ˆä¸å†å•ç‹¬è°ƒç”¨LLMï¼‰
            logger.info("æ­¥éª¤ 3: æå–éªŒè¯åçš„å¤šåª’ä½“å†…å®¹...")
            media_result = self._extract_media_from_validation(validation_result)

            # æ­¥éª¤ 4: æŠ¥å‘Šç”Ÿæˆ
            logger.info("æ­¥éª¤ 4: æŠ¥å‘Šç”Ÿæˆ...")
            report = self._step4_report_generation(
                context, event_info, extraction_result, validation_result, media_result
            )

            return {
                "validation": validation_result,
                "extraction": extraction_result,
                "media": media_result,
                "report": report,
            }
        except Exception as e:
            logger.exception("LLM å¤„ç†å¤±è´¥: %s", e)
            raise

    def _pre_filter_results(
        self,
        all_items: List[Dict[str, Any]],
        event_info: Dict[str, Any],
    ) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """é¢„è¿‡æ»¤æœç´¢ç»“æœï¼Œç§»é™¤æ˜æ˜¾ä¸ç›¸å…³çš„ç»“æœã€‚
        
        Returns:
            (filtered_items, filter_details): è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨å’Œè¿‡æ»¤è¯¦æƒ…åˆ—è¡¨
        """
        from datetime import datetime, timedelta
        
        filtered = []
        filter_details = []  # è®°å½•è¿‡æ»¤è¯¦æƒ…
        event_time_str = event_info.get("event_time", "")
        province = (event_info.get("province", "") or "").lower()
        country = (event_info.get("country", "") or "").lower()
        rain_term = (event_info.get("rain_term", "rain") or "rain").lower()
        flood_term = (event_info.get("flood_term", "flood") or "flood").lower()
        
        # è§£æäº‹ä»¶æ—¶é—´
        event_time = None
        if event_time_str:
            try:
                event_time = datetime.strptime(event_time_str, "%Y-%m-%d %H:%M:%S")
            except (ValueError, TypeError):
                try:
                    event_time = datetime.fromisoformat(event_time_str.replace("Z", ""))
                except (ValueError, TypeError):
                    pass
        
        time_window_days = self.config.PRE_FILTER_TIME_WINDOW_DAYS
        is_strict = self.config.PRE_FILTER_MODE == "strict"
        
        for idx, item in enumerate(all_items):
            checks = {
                "time": False,
                "location": False,
                "keyword": False,
            }
            reasons = []
            
            # 1. æ—¶é—´è¿‡æ»¤
            if event_time:
                checks["time"] = self._check_time_match(item, event_time, time_window_days)
                if not checks["time"]:
                    reasons.append("æ—¶é—´ä¸åŒ¹é…")
            else:
                checks["time"] = True  # å¦‚æœæ²¡æœ‰äº‹ä»¶æ—¶é—´ï¼Œè·³è¿‡æ—¶é—´æ£€æŸ¥
            
            # 2. åœ°ç‚¹è¿‡æ»¤
            if province or country:
                checks["location"] = self._check_location_match(item, province, country)
                if not checks["location"]:
                    reasons.append("åœ°ç‚¹ä¸åŒ¹é…")
            else:
                checks["location"] = True  # å¦‚æœæ²¡æœ‰åœ°ç‚¹ä¿¡æ¯ï¼Œè·³è¿‡åœ°ç‚¹æ£€æŸ¥
            
            # 3. å…³é”®è¯è¿‡æ»¤
            checks["keyword"] = self._check_keyword_match(item, rain_term, flood_term)
            if not checks["keyword"]:
                reasons.append("å…³é”®è¯ä¸åŒ¹é…")
            
            # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä¿ç•™
            should_keep = False
            if is_strict:
                # ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³æ‰€æœ‰æ¡ä»¶
                should_keep = all(checks.values())
            else:
                # å®½æ¾æ¨¡å¼ï¼šæ»¡è¶³ä»»æ„ä¸€ä¸ªæ¡ä»¶å³å¯
                should_keep = any(checks.values())
            
            if should_keep:
                filtered.append(item)
            else:
                # è®°å½•è¢«è¿‡æ»¤çš„é¡¹
                filter_details.append({
                    "index": idx,
                    "title": item.get("title", "N/A")[:100],  # é™åˆ¶é•¿åº¦
                    "url": item.get("url", "N/A"),
                    "checks": checks,
                    "reasons": reasons,
                    "mode": "strict" if is_strict else "loose",
                })
        
        return filtered, filter_details
    
    def _pre_filter_with_media_priority(
        self,
        all_items: List[Dict[str, Any]],
        event_info: Dict[str, Any],
    ) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """é¢„è¿‡æ»¤æœç´¢ç»“æœï¼Œåª’ä½“ä¼˜å…ˆè¿›å…¥15æ¡ï¼ˆæœ€å¤š3æ¡åª’ä½“ï¼‰ã€‚
        
        Returns:
            (filtered_items, filter_details): è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨ï¼ˆæœ€å¤š15æ¡ï¼Œåª’ä½“ä¼˜å…ˆï¼‰å’Œè¿‡æ»¤è¯¦æƒ…åˆ—è¡¨
        """
        # 1. åˆ†ç¦»åª’ä½“å’Œæ–°é—»
        media_items = [item for item in all_items if item.get("channel") in {"media", "social"}]
        news_items = [item for item in all_items if item.get("channel") not in {"media", "social"}]
        
        # 2. åˆ†åˆ«ç­›é€‰ï¼ˆæ—¶é—´+åœ°ç‚¹+å…³é”®è¯ï¼‰
        filtered_media, media_filter_details = self._pre_filter_results(media_items, event_info)
        filtered_news, news_filter_details = self._pre_filter_results(news_items, event_info)
        
        # 3. åª’ä½“ä¼˜å…ˆï¼šæœ€å¤šå–3æ¡ï¼ˆå¦‚æœæœ‰ï¼‰
        selected_media = filtered_media[:3] if len(filtered_media) >= 3 else filtered_media
        
        # 4. æ–°é—»è¡¥å……ï¼šå–å‰©ä½™æ•°é‡ï¼ˆ15 - åª’ä½“æ•°é‡ï¼‰
        remaining_count = 15 - len(selected_media)
        selected_news = filtered_news[:remaining_count] if len(filtered_news) >= remaining_count else filtered_news
        
        # 5. åˆå¹¶ï¼ˆä¸æ’åºï¼Œè®©LLM1æ’åºï¼‰
        filtered_items = selected_media + selected_news
        
        # 6. åˆå¹¶è¿‡æ»¤è¯¦æƒ…
        all_filter_details = media_filter_details + news_filter_details
        
        logger.info(
            "é¢„è¿‡æ»¤å®Œæˆï¼šåª’ä½“ %s æ¡ï¼ˆä¼˜å…ˆä¿ç•™ %s æ¡ï¼‰ï¼Œæ–°é—» %s æ¡ï¼Œæ€»è®¡ %s æ¡",
            len(filtered_media),
            len(selected_media),
            len(selected_news),
            len(filtered_items),
        )
        
        return filtered_items, all_filter_details
    
    def _extract_date_from_url(self, url: str) -> Optional[datetime]:
        """ä»URLä¸­æå–æ—¥æœŸï¼ˆå¸¸è§æ ¼å¼ï¼š/2025/10/27/ æˆ– /2025-10-27/ï¼‰ã€‚"""
        if not url:
            return None
        
        import re
        # åŒ¹é… URL ä¸­çš„æ—¥æœŸæ ¼å¼ï¼š/2025/10/27/ æˆ– /2025-10-27/
        patterns = [
            r'/(\d{4})/(\d{1,2})/(\d{1,2})/',  # /2025/10/27/
            r'/(\d{4})-(\d{1,2})-(\d{1,2})/',  # /2025-10-27/
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                try:
                    year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                    return datetime(year, month, day)
                except (ValueError, TypeError):
                    continue
        
        return None
    
    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸï¼ˆå¸¸è§æ ¼å¼ï¼šOctober 29, 2025 æˆ– 10/29/2025ï¼‰ã€‚"""
        if not text:
            return None
        
        import re
        
        month_names = {
            'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6,
            'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12,
            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
            'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12,
        }
        
        # æ¨¡å¼1: October 29, 2025 æˆ– Oct 29, 2025
        pattern1 = r'(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),\s+(\d{4})'
        match = re.search(pattern1, text, re.IGNORECASE)
        if match:
            try:
                month_str = match.group(0).split()[0].lower()
                day = int(match.group(1))
                year = int(match.group(2))
                month = month_names.get(month_str)
                if month:
                    return datetime(year, month, day)
            except (ValueError, TypeError, IndexError):
                pass
        
        # æ¨¡å¼2: 2025-10-29
        pattern2 = r'(\d{4})-(\d{1,2})-(\d{1,2})'
        match = re.search(pattern2, text)
        if match:
            try:
                year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                return datetime(year, month, day)
            except (ValueError, TypeError):
                pass
        
        # æ¨¡å¼3: 10/29/2025 æˆ– 10-29-2025ï¼ˆæ³¨æ„ï¼šå¯èƒ½æ˜¯ MM/DD/YYYY æˆ– DD/MM/YYYYï¼‰
        pattern3 = r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})'
        match = re.search(pattern3, text)
        if match:
            try:
                # å°è¯• MM/DD/YYYY
                month, day, year = int(match.group(1)), int(match.group(2)), int(match.group(3))
                if 1 <= month <= 12 and 1 <= day <= 31:
                    return datetime(year, month, day)
            except (ValueError, TypeError):
                pass
        
        return None

    def _check_time_match(
        self,
        item: Dict[str, Any],
        event_time: datetime,
        window_days: int,
    ) -> bool:
        """æ£€æŸ¥æ—¶é—´æ˜¯å¦åŒ¹é…ï¼ˆåªä¿ç•™äº‹ä»¶æ—¶é—´ + N å¤©å†…çš„ç»“æœï¼‰ã€‚"""
        published_at = item.get("published_at")
        pub_time = None
        
        # 1. ä¼˜å…ˆä½¿ç”¨ published_at å­—æ®µ
        if published_at:
            try:
                if isinstance(published_at, str):
                    # å°è¯•è§£æISOæ ¼å¼
                    published_at_clean = published_at.replace("Z", "+00:00")
                    pub_time = datetime.fromisoformat(published_at_clean)
                else:
                    pub_time = published_at
            except (ValueError, TypeError, AttributeError):
                pass
        
        # 2. å¦‚æœ published_at ä¸ºç©ºï¼Œå°è¯•ä» URL ä¸­æå–æ—¥æœŸ
        if not pub_time:
            url = item.get("url", "")
            pub_time = self._extract_date_from_url(url)
        
        # 3. å¦‚æœ URL ä¹Ÿæ²¡æœ‰ï¼Œå°è¯•ä»æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–æ—¥æœŸ
        if not pub_time:
            title = item.get("title", "")
            summary = item.get("summary", "") or item.get("description", "")
            text = f"{title} {summary}"
            pub_time = self._extract_date_from_text(text)
        
        # 4. å¦‚æœä»ç„¶æ²¡æœ‰æ—¥æœŸï¼Œä¿ç•™ï¼ˆè®©LLMåˆ¤æ–­ï¼‰
        if not pub_time:
            return True
        
        try:
            # è½¬æ¢ä¸ºUTCï¼ˆå¦‚æœæœ‰æ—¶åŒºä¿¡æ¯ï¼‰
            if pub_time.tzinfo:
                pub_time = pub_time.replace(tzinfo=None)
            if event_time.tzinfo:
                event_time = event_time.replace(tzinfo=None)
            
            # åªä¿ç•™äº‹ä»¶æ—¶é—´ä¹‹åçš„å†…å®¹ï¼ˆäº‹ä»¶æ—¶é—´ + N å¤©å†…ï¼‰
            # å‘å¸ƒæ—¶é—´å¿…é¡»åœ¨äº‹ä»¶æ—¶é—´ä¹‹åï¼Œä¸”åœ¨äº‹ä»¶æ—¶é—´ + window_days å¤©å†…
            if pub_time < event_time:
                return False  # å‘å¸ƒæ—¶é—´åœ¨äº‹ä»¶æ—¶é—´ä¹‹å‰ï¼Œè¿‡æ»¤æ‰
            
            time_diff = (pub_time - event_time).days
            return time_diff <= window_days
        except (ValueError, TypeError, AttributeError):
            return True  # è§£æå¤±è´¥ï¼Œä¿ç•™ï¼ˆè®©LLMåˆ¤æ–­ï¼‰
    
    def _check_location_match(
        self,
        item: Dict[str, Any],
        province: str,
        country: str,
    ) -> bool:
        """æ£€æŸ¥åœ°ç‚¹æ˜¯å¦åŒ¹é…ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦ä¸­åŒ…å«çœåæˆ–å›½å®¶åï¼‰ã€‚"""
        title = (item.get("title", "") or "").lower()
        summary = (item.get("summary", "") or item.get("description", "") or "").lower()
        text = f"{title} {summary}"
        
        if province and province in text:
            return True
        if country and country in text:
            return True
        return False
    
    def _check_keyword_match(
        self,
        item: Dict[str, Any],
        rain_term: str,
        flood_term: str,
    ) -> bool:
        """æ£€æŸ¥å…³é”®è¯æ˜¯å¦åŒ¹é…ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦ä¸­åŒ…å«ç¾å®³å…³é”®è¯ï¼‰ã€‚"""
        title = (item.get("title", "") or "").lower()
        summary = (item.get("summary", "") or item.get("description", "") or "").lower()
        text = f"{title} {summary}"
        
        if rain_term and rain_term in text:
            return True
        if flood_term and flood_term in text:
            return True
        return False
    
    def _prepare_event_info(self, context: EventContext) -> Dict[str, Any]:
        """å‡†å¤‡äº‹ä»¶ä¿¡æ¯ã€‚"""
        event = context.rain_event
        profile = context.location_profile or {}

        return {
            "event_id": event.event_id or "",
            "event_time": (
                event.event_time.strftime("%Y-%m-%d %H:%M:%S")
                if event.event_time
                else ""
            ),
            "location": event.location_name or "",
            "province": event.extras.get("province", ""),
            "country": event.country or "",
            "rainfall_mm": event.rainfall_mm or 0,
            "rain_term": profile.get("rain_term") or "rain",  # å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼
            "flood_term": profile.get("flood_term") or "flood",  # å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼
        }

    def _save_raw_items_before_filter(
        self,
        all_items: List[Dict[str, Any]],
        context: EventContext,
        event_info: Dict[str, Any],
    ):
        """ä¿å­˜é¢„è¿‡æ»¤å‰çš„åŸå§‹æœç´¢ç»“æœåˆ°æ–‡ä»¶ã€‚"""
        try:
            from pathlib import Path
            from datetime import datetime
            import json
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šsearch_outputs/YYYYMMDD/
            # ä» event_id æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå‰8ä½ï¼šYYYYMMDDï¼‰
            event_id = context.rain_event.event_id
            event_id_str = str(event_id)
            date_dir = event_id_str[:8] if len(event_id_str) >= 8 and event_id_str[:8].isdigit() else ""
            if not date_dir:
                # å¦‚æœæ— æ³•ä» ID æå–ï¼Œå°è¯•ä» event_time è·å–
                if context.rain_event.event_time:
                    date_dir = context.rain_event.event_time.strftime("%Y%m%d")
                else:
                    date_dir = "unknown"
            
            output_dir = Path("search_outputs") / date_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨å®Œæ•´äº‹ä»¶IDï¼Œæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_event_id = event_id.replace("/", "_").replace("\\", "_")
            filename = f"{safe_event_id}_raw_items_before_filter.md"
            filepath = output_dir / filename
            
            # ç”ŸæˆMarkdownå†…å®¹
            content = f"""# é¢„è¿‡æ»¤å‰çš„åŸå§‹æœç´¢ç»“æœ

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## äº‹ä»¶ä¿¡æ¯

- **äº‹ä»¶ID**: {event_info.get('event_id', 'N/A')}
- **æ—¶é—´**: {event_info.get('event_time', 'N/A')}
- **åœ°ç‚¹**: {event_info.get('location', 'N/A')} ({event_info.get('province', 'N/A')}, {event_info.get('country', 'N/A')})
- **é™é›¨é‡**: {event_info.get('rainfall_mm', 'N/A')}mm

## åŸå§‹æœç´¢ç»“æœç»Ÿè®¡

- **æ€»æ•°é‡**: {len(all_items)} æ¡
- **æ¥æºæ¸ é“**: {', '.join(context.raw_contents.keys()) if context.raw_contents else 'N/A'}

---

## åŸå§‹æœç´¢ç»“æœè¯¦æƒ…

"""
            
            # æ·»åŠ æ¯æ¡ç»“æœ
            for idx, item in enumerate(all_items, 1):
                title = item.get("title", "N/A")
                url = item.get("url", "N/A")
                summary = item.get("summary") or item.get("description", "N/A")
                published_at = item.get("published_at", "N/A")
                source = item.get("source", "N/A")
                channel = item.get("channel", "N/A")
                
                # å¤„ç†æ‘˜è¦é•¿åº¦
                if isinstance(summary, str) and len(summary) > 300:
                    summary_display = summary[:300] + "..."
                else:
                    summary_display = summary
                
                content += f"""### ç»“æœ {idx}

**æ ‡é¢˜**: {title}

**URL**: {url}

**æ‘˜è¦**: {summary_display}

**å‘å¸ƒæ—¶é—´**: {published_at}

**æ¥æº**: {source}

**æ¸ é“**: {channel}

**å®Œæ•´æ•°æ®**:
```json
{json.dumps(item, indent=2, ensure_ascii=False)}
```

---

"""
            
            # ä¿å­˜æ–‡ä»¶
            filepath.write_text(content, encoding="utf-8")
            logger.info("âœ… é¢„è¿‡æ»¤å‰çš„åŸå§‹æœç´¢ç»“æœå·²ä¿å­˜åˆ°: %s", filepath)
            
        except Exception as e:
            logger.warning("ä¿å­˜åŸå§‹æœç´¢ç»“æœå¤±è´¥: %s", e)

    def _save_filtered_items_after_prefilter(
        self,
        filtered_items: List[Dict[str, Any]],
        context: EventContext,
        event_info: Dict[str, Any],
    ):
        """ä¿å­˜é¢„è¿‡æ»¤åçš„ç»“æœåˆ°æ–‡ä»¶ï¼ˆé€šè¿‡æ—¥æœŸã€åœ°ç‚¹ã€å…³é”®è¯åˆç­›åçš„å†…å®¹ï¼‰ã€‚"""
        try:
            from pathlib import Path
            from datetime import datetime
            import json
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šsearch_outputs/YYYYMMDD/
            # ä» event_id æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå‰8ä½ï¼šYYYYMMDDï¼‰
            event_id = context.rain_event.event_id
            event_id_str = str(event_id)
            date_dir = event_id_str[:8] if len(event_id_str) >= 8 and event_id_str[:8].isdigit() else ""
            if not date_dir:
                # å¦‚æœæ— æ³•ä» ID æå–ï¼Œå°è¯•ä» event_time è·å–
                if context.rain_event.event_time:
                    date_dir = context.rain_event.event_time.strftime("%Y%m%d")
                else:
                    date_dir = "unknown"
            
            output_dir = Path("search_outputs") / date_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨å®Œæ•´äº‹ä»¶IDï¼Œæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_event_id = event_id.replace("/", "_").replace("\\", "_")
            filename = f"{safe_event_id}_filtered_items_after_prefilter.md"
            filepath = output_dir / filename
            
            # ç”ŸæˆMarkdownå†…å®¹
            content = f"""# é¢„è¿‡æ»¤åçš„æœç´¢ç»“æœï¼ˆåˆç­›åï¼‰

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## äº‹ä»¶ä¿¡æ¯

- **äº‹ä»¶ID**: {event_info.get('event_id', 'N/A')}
- **æ—¶é—´**: {event_info.get('event_time', 'N/A')}
- **åœ°ç‚¹**: {event_info.get('location', 'N/A')} ({event_info.get('province', 'N/A')}, {event_info.get('country', 'N/A')})
- **é™é›¨é‡**: {event_info.get('rainfall_mm', 'N/A')}mm

## é¢„è¿‡æ»¤é…ç½®

- **é¢„è¿‡æ»¤æ¨¡å¼**: {self.config.PRE_FILTER_MODE}
- **æ—¶é—´çª—å£**: äº‹ä»¶æ—¶é—´ + {self.config.PRE_FILTER_TIME_WINDOW_DAYS} å¤©
- **è¿‡æ»¤è§„åˆ™**: 
  - âœ“ æ—¶é—´åŒ¹é…ï¼ˆåªä¿ç•™äº‹ä»¶æ—¶é—´ + {self.config.PRE_FILTER_TIME_WINDOW_DAYS} å¤©å†…çš„ç»“æœï¼‰
  - âœ“ åœ°ç‚¹åŒ¹é…ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«çœåæˆ–å›½å®¶åï¼‰
  - âœ“ å…³é”®è¯åŒ¹é…ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«"rain"/"flood"ç­‰å…³é”®è¯ï¼‰

## é¢„è¿‡æ»¤åç»“æœç»Ÿè®¡

- **æ•°é‡**: {len(filtered_items)} æ¡
- **å°†äº¤ç»™LLMéªŒè¯**: æœ€å¤š {self.config.MAX_ITEMS_FOR_LLM_VALIDATION} æ¡

---

## é¢„è¿‡æ»¤åçš„æœç´¢ç»“æœè¯¦æƒ…

"""
            
            # æ·»åŠ æ¯æ¡ç»“æœ
            for idx, item in enumerate(filtered_items, 1):
                title = item.get("title", "N/A")
                url = item.get("url", "N/A")
                summary = item.get("summary") or item.get("description", "N/A")
                published_at = item.get("published_at", "N/A")
                source = item.get("source", "N/A")
                channel = item.get("channel", "N/A")
                
                # å¤„ç†æ‘˜è¦é•¿åº¦
                if isinstance(summary, str) and len(summary) > 300:
                    summary_display = summary[:300] + "..."
                else:
                    summary_display = summary
                
                # æ ‡è®°æ˜¯å¦ä¼šè¢«äº¤ç»™LLM
                will_send_to_llm = idx <= self.config.MAX_ITEMS_FOR_LLM_VALIDATION
                llm_marker = "âœ… å°†äº¤ç»™LLMéªŒè¯" if will_send_to_llm else f"âš ï¸ è¶…å‡ºé™åˆ¶ï¼ˆåªå–å‰{self.config.MAX_ITEMS_FOR_LLM_VALIDATION}æ¡ï¼‰"
                
                content += f"""### ç»“æœ {idx} {llm_marker}

**æ ‡é¢˜**: {title}

**URL**: {url}

**æ‘˜è¦**: {summary_display}

**å‘å¸ƒæ—¶é—´**: {published_at}

**æ¥æº**: {source}

**æ¸ é“**: {channel}

**å®Œæ•´æ•°æ®**:
```json
{json.dumps(item, indent=2, ensure_ascii=False)}
```

---

"""
            
            # ä¿å­˜æ–‡ä»¶
            filepath.write_text(content, encoding="utf-8")
            logger.info("âœ… é¢„è¿‡æ»¤åçš„æœç´¢ç»“æœå·²ä¿å­˜åˆ°: %s", filepath)
            
        except Exception as e:
            logger.warning("ä¿å­˜é¢„è¿‡æ»¤åçš„æœç´¢ç»“æœå¤±è´¥: %s", e)

    def _generate_minimal_report_no_relevant(
        self,
        context: EventContext,
        event_info: Dict[str, Any],
    ) -> str:
        """ç”Ÿæˆæœ€å°æŠ¥å‘Šï¼ˆéªŒè¯åæ²¡æœ‰ç›¸å…³ç»“æœæ—¶ï¼‰ã€‚"""
        event = context.rain_event
        profile = context.location_profile or {}
        rain_term = profile.get("rain_term", "rain")
        flood_term = profile.get("flood_term", "flood")
        
        report = f"""# Flood Event Report: {event.location_name}, {event.country}

## 1. Event Overview

On {event.event_time.strftime('%B %d, %Y') if event.event_time else 'N/A'}, {event.location_name}, located in {event.extras.get('province', '')}, {event.country}, experienced a rainfall event with {event.rainfall_mm}mm of precipitation.

*   **Local Terminology:**
    *   Rain: "{rain_term}"
    *   Flood: "{flood_term}"

## 2. Flood Timeline

No timeline information is available. After pre-filtering and LLM validation, no relevant news or media sources were found for this event.

## 3. Multimedia & News Sources

No multimedia content or news sources were found for this event. This may be due to:
- The search results did not match the event criteria (time, location, keywords)
- Limited media coverage of the event
- The event may not have generated significant news coverage

## 4. Impact Assessment

No impact assessment data is available as no relevant sources were found for this event.

## 5. Summary

This rainfall event in {event.location_name}, {event.country}, recorded {event.rainfall_mm}mm of precipitation. However, after automated filtering and validation, no relevant news coverage, media sources, or detailed information were found to provide a comprehensive analysis of the event's impact, timeline, or consequences. This may indicate that the event did not generate significant media attention or that the available search results did not match the event criteria.
"""
        return report

    def _save_validation_results(
        self,
        relevant_items: List[Dict[str, Any]],
        irrelevant_items: List[Dict[str, Any]],
        context: EventContext,
        event_info: Dict[str, Any],
    ):
        """ä¿å­˜ Gemini éªŒè¯ç»“æœåˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬è¢«æ’é™¤é¡¹çš„åŸå› ï¼‰ã€‚"""
        try:
            from pathlib import Path
            from datetime import datetime
            import json
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šsearch_outputs/YYYYMMDD/
            # ä» event_id æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå‰8ä½ï¼šYYYYMMDDï¼‰
            event_id = context.rain_event.event_id
            event_id_str = str(event_id)
            date_dir = event_id_str[:8] if len(event_id_str) >= 8 and event_id_str[:8].isdigit() else ""
            if not date_dir:
                # å¦‚æœæ— æ³•ä» ID æå–ï¼Œå°è¯•ä» event_time è·å–
                if context.rain_event.event_time:
                    date_dir = context.rain_event.event_time.strftime("%Y%m%d")
                else:
                    date_dir = "unknown"
            
            output_dir = Path("search_outputs") / date_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨å®Œæ•´äº‹ä»¶IDï¼Œæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_event_id = event_id.replace("/", "_").replace("\\", "_")
            filename = f"{safe_event_id}_llm_validation_results.md"
            filepath = output_dir / filename
            
            # ç”ŸæˆMarkdownå†…å®¹
            content = f"""# LLM éªŒè¯ç»“æœï¼ˆGemini åˆ¤æ–­ï¼‰

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## äº‹ä»¶ä¿¡æ¯

- **äº‹ä»¶ID**: {event_info.get('event_id', 'N/A')}
- **æ—¶é—´**: {event_info.get('event_time', 'N/A')}
- **åœ°ç‚¹**: {event_info.get('location', 'N/A')} ({event_info.get('province', 'N/A')}, {event_info.get('country', 'N/A')})
- **é™é›¨é‡**: {event_info.get('rainfall_mm', 'N/A')}mm

## éªŒè¯ç»Ÿè®¡

- **ç›¸å…³ç»“æœ**: {len(relevant_items)} æ¡
- **ä¸ç›¸å…³ç»“æœ**: {len(irrelevant_items)} æ¡

---

## âœ… ç›¸å…³ç»“æœï¼ˆ{len(relevant_items)} æ¡ï¼‰

"""
            
            if not relevant_items:
                content += "æ— ç›¸å…³ç»“æœã€‚\n\n"
            else:
                for idx, item in enumerate(relevant_items, 1):
                    title = item.get("title", "N/A")
                    url = item.get("url", "N/A")
                    relevance_score = item.get("relevance_score", "N/A")
                    reason = item.get("reason", "N/A")
                    
                    content += f"""### ç›¸å…³é¡¹ {idx}

**æ ‡é¢˜**: {title}

**URL**: {url}

**ç›¸å…³æ€§è¯„åˆ†**: {relevance_score}

**åˆ¤æ–­åŸå› **: {reason}

---

"""
            
            content += f"""
## âŒ ä¸ç›¸å…³ç»“æœï¼ˆ{len(irrelevant_items)} æ¡ï¼‰- Gemini æ’é™¤çš„åŸå› 

"""
            
            if not irrelevant_items:
                content += "æ— ä¸ç›¸å…³ç»“æœã€‚\n\n"
            else:
                content += "ä»¥ä¸‹æ˜¯ Gemini åˆ¤æ–­ä¸ºä¸ç›¸å…³çš„ç»“æœåŠå…¶æ’é™¤åŸå› ï¼š\n\n"
                
                for idx, item in enumerate(irrelevant_items, 1):
                    original_index = item.get("index", "N/A")
                    title = item.get("title", "N/A")
                    url = item.get("url", "N/A")
                    reason = item.get("reason", "N/A")
                    
                    content += f"""### ä¸ç›¸å…³é¡¹ {idx} (åŸå§‹ç´¢å¼•: {original_index})

**æ ‡é¢˜**: {title}

**URL**: {url}

**âŒ æ’é™¤åŸå› **: {reason}

---

"""
            
            content += f"""
## ğŸ“ è¯´æ˜

æ­¤æŠ¥å‘Šå±•ç¤ºäº† Gemini LLM éªŒè¯æ­¥éª¤çš„è¯¦ç»†ç»“æœï¼š

1. **éªŒè¯ç›®çš„**: æ™ºèƒ½åˆ¤æ–­æœç´¢ç»“æœæ˜¯å¦å±äºè¯¥ç‰¹å®šäº‹ä»¶
2. **åˆ¤æ–­æ ‡å‡†**:
   - æ—¶é—´æ˜¯å¦åŒ¹é…ï¼ˆäº‹ä»¶æ—¶é—´ Â± 3å¤©ï¼‰
   - åœ°ç‚¹æ˜¯å¦åŒ¹é…ï¼ˆçœçº§æˆ–å¸‚çº§ï¼‰
   - å†…å®¹æ˜¯å¦ç›¸å…³ï¼ˆé™é›¨ã€æ´ªæ°´ã€ç¾å®³ï¼‰
3. **ä¸ç›¸å…³é¡¹**: å¦‚æœ Gemini åˆ¤æ–­æŸæ¡ç»“æœä¸ç›¸å…³ï¼Œä¼šæä¾›è¯¦ç»†çš„æ’é™¤åŸå› 

## ğŸ”— ç›¸å…³æ–‡ä»¶

- é¢„è¿‡æ»¤å‰åŸå§‹ç»“æœ: `{safe_event_id}_raw_items_before_filter.md`
- é¢„è¿‡æ»¤åç»“æœ: `{safe_event_id}_filtered_items_after_prefilter.md`
- è¯¦ç»†æ—¥å¿—: `test_log.md`
"""
            
            # ä¿å­˜æ–‡ä»¶
            filepath.write_text(content, encoding="utf-8")
            logger.info("âœ… LLM éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: %s", filepath)
            logger.info("   ç›¸å…³: %s æ¡, ä¸ç›¸å…³: %s æ¡", len(relevant_items), len(irrelevant_items))
            
        except Exception as e:
            logger.warning("ä¿å­˜LLMéªŒè¯ç»“æœå¤±è´¥: %s", e)

    def _step1_validation(
        self, context: EventContext, event_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ­¥éª¤ 1: äº‹ä»¶éªŒè¯å’Œå†²çªè§£å†³ã€‚"""
        # æ”¶é›†æ‰€æœ‰åŸå§‹æœç´¢ç»“æœï¼Œå¹¶æ·»åŠ channelä¿¡æ¯
        all_items: List[Dict[str, Any]] = []
        for channel, items in (context.raw_contents or {}).items():
            for item in items or []:
                # ç¡®ä¿æ¯ä¸ªiteméƒ½æœ‰channelä¿¡æ¯ï¼Œæ–¹ä¾¿åç»­è¯†åˆ«å¤šåª’ä½“
                item_with_channel = {**item, "channel": channel}
                all_items.append(item_with_channel)

        if not all_items:
            logger.warning("æ²¡æœ‰æœç´¢ç»“æœéœ€è¦éªŒè¯")
            return {
                "relevant_items": [],
                "irrelevant_items": [],
            }

        # ä¿å­˜åŸå§‹æœç´¢ç»“æœåˆ°æ–‡ä»¶ï¼ˆé¢„è¿‡æ»¤å‰ï¼‰
        self._save_raw_items_before_filter(all_items, context, event_info)

        # é¢„è¿‡æ»¤ï¼šåœ¨äº¤ç»™LLMå‰è¿›è¡Œç®€å•è§„åˆ™åˆ¤æ–­ï¼Œåª’ä½“ä¼˜å…ˆè¿›å…¥15æ¡
        if self.config.PRE_FILTER_ENABLED:
            original_count = len(all_items)
            all_items, filter_details = self._pre_filter_with_media_priority(all_items, event_info)
            filtered_count = len(all_items)
            if original_count > filtered_count:
                logger.info(
                    "é¢„è¿‡æ»¤ï¼šä» %s æ¡ç»“æœä¸­è¿‡æ»¤å‡º %s æ¡ç›¸å…³ç»“æœï¼ˆç§»é™¤äº† %s æ¡ä¸ç›¸å…³ç»“æœï¼Œåª’ä½“ä¼˜å…ˆï¼‰",
                    original_count,
                    filtered_count,
                    original_count - filtered_count,
                )
                # è®°å½•è¯¦ç»†æ—¥å¿—
                from ..utils.detailed_logger import get_detailed_logger
                detailed_logger = get_detailed_logger()
                detailed_logger.log_pre_filter_results(
                    original_count=original_count,
                    filtered_count=filtered_count,
                    filter_details=filter_details,
                )
                
                # ä¿å­˜é¢„è¿‡æ»¤åçš„ç»“æœåˆ°æ–‡ä»¶
                self._save_filtered_items_after_prefilter(all_items, context, event_info)
        
        # é¢„è¿‡æ»¤ååº”è¯¥å·²ç»æ˜¯15æ¡äº†ï¼ˆåª’ä½“ä¼˜å…ˆï¼‰ï¼Œä¸éœ€è¦å†æ¬¡é™åˆ¶
        # ä½†å¦‚æœè¶…è¿‡15æ¡ï¼Œé™åˆ¶ä¸º15æ¡
        if len(all_items) > 15:
            logger.warning(
                "é¢„è¿‡æ»¤åç»“æœè¿‡å¤šï¼ˆ%sæ¡ï¼‰ï¼Œé™åˆ¶ä¸º15æ¡",
                len(all_items),
            )
            all_items = all_items[:15]

        # æ„å»º promptï¼ˆåªä½¿ç”¨ç®€çŸ­ä¿¡æ¯ï¼šæ ‡é¢˜+æ—¥æœŸ+æ‘˜è¦200å­—ç¬¦ï¼‰
        time_window_days = self.config.LLM_VALIDATION_TIME_WINDOW_DAYS
        messages = build_validation_prompt(event_info, all_items, time_window_days)

        # è°ƒç”¨ LLM
        client = self._get_client()
        response = client.chat(
            messages=messages,
            temperature=self.config.LLM_TEMPERATURE,
            max_tokens=self.config.LLM_MAX_TOKENS,
            response_format={"type": "json_object"} if self.config.LLM_PROVIDER == "openai" else None,
        )

        # è§£æå“åº”
        result = client.parse_json_response(response)

        # æ˜ å°„å›åŸå§‹æ•°æ®ï¼Œå¹¶å®ç°åª’ä½“ä¼˜å…ˆ
        relevant_items_raw = result.get("relevant_items", [])
        
        # åˆ†ç¦»åª’ä½“å’Œæ–°é—»
        media_items = []
        news_items = []
        for item in relevant_items_raw:
            idx = item.get("index", -1)
            if 0 <= idx < len(all_items):
                original = all_items[idx]
                item_with_score = {
                    **original,
                    "relevance_score": item.get("relevance_score", 0.0),
                    "reason": item.get("reason", ""),
                }
                # åˆ¤æ–­æ˜¯å¦æ˜¯åª’ä½“
                channel = original.get("channel", "")
                if channel in {"media", "social"}:
                    media_items.append(item_with_score)
                else:
                    news_items.append(item_with_score)
        
        # åª’ä½“ä¼˜å…ˆï¼šå³ä½¿è¯„åˆ†ä½ä¹Ÿä¼˜å…ˆä¿ç•™ï¼ˆæœ€å¤š3æ¡ï¼‰
        # å¦‚æœåª’ä½“åœ¨å‰10æ¡ä¸­ï¼Œä¼˜å…ˆä¿ç•™ï¼›å¦‚æœæ²¡æœ‰ï¼Œå°±è·³è¿‡
        selected_media = media_items[:3] if len(media_items) >= 3 else media_items
        
        # æ–°é—»è¡¥å……ï¼šå–å‰©ä½™æ•°é‡ï¼ˆ10 - åª’ä½“æ•°é‡ï¼‰
        remaining_count = 10 - len(selected_media)
        selected_news = news_items[:remaining_count] if len(news_items) >= remaining_count else news_items
        
        # åˆå¹¶ï¼ˆåª’ä½“ä¼˜å…ˆï¼‰
        relevant_items = selected_media + selected_news
        
        irrelevant_items = result.get("irrelevant_items", [])
        logger.info(
            "éªŒè¯å®Œæˆ: %s æ¡ç›¸å…³ï¼ˆåª’ä½“ %s æ¡ï¼Œæ–°é—» %s æ¡ï¼‰ï¼Œ%s æ¡ä¸ç›¸å…³",
            len(relevant_items),
            len(selected_media),
            len(selected_news),
            len(irrelevant_items),
        )

        # ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬è¢«æ’é™¤é¡¹çš„åŸå› ï¼‰
        self._save_validation_results(
            relevant_items=relevant_items,
            irrelevant_items=irrelevant_items,
            context=context,
            event_info=event_info,
        )

        return {
            "relevant_items": relevant_items,
            "irrelevant_items": irrelevant_items,
        }

    def _step2_extraction(
        self,
        context: EventContext,
        event_info: Dict[str, Any],
        validation_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """æ­¥éª¤ 2: æ—¶é—´çº¿å’Œå½±å“æå–ã€‚"""
        verified_items = validation_result.get("relevant_items", [])

        if not verified_items:
            logger.warning("æ²¡æœ‰éªŒè¯åçš„ä¿¡æ¯éœ€è¦æå–")
            return {"timeline": [], "impact": {}}

        # æ„å»º prompt
        messages = build_extraction_prompt(event_info, verified_items)
        
        # è®°å½• LLM è¯·æ±‚
        from ..utils.detailed_logger import get_detailed_logger
        detailed_logger = get_detailed_logger()
        detailed_logger.log_llm_request(
            step="æ—¶é—´çº¿å’Œå½±å“æå–",
            step_number=2,
            provider=self.config.LLM_PROVIDER,
            model=self.config.OPENAI_MODEL if self.config.LLM_PROVIDER == "openai" else self.config.GEMINI_MODEL,
            prompt_messages=messages,
            config={
                "temperature": self.config.LLM_TEMPERATURE,
                "max_tokens": self.config.LLM_MAX_TOKENS,
                "response_format": {"type": "json_object"} if self.config.LLM_PROVIDER == "openai" else None,
            },
        )

        # è°ƒç”¨ LLM
        client = self._get_client()
        response = client.chat(
            messages=messages,
            temperature=self.config.LLM_TEMPERATURE,
            max_tokens=self.config.LLM_MAX_TOKENS,
            response_format={"type": "json_object"} if self.config.LLM_PROVIDER == "openai" else None,
        )

        # è§£æå“åº”
        result = client.parse_json_response(response)
        
        # è®°å½• LLM å“åº”
        detailed_logger.log_llm_response(
            step="æ—¶é—´çº¿å’Œå½±å“æå–",
            step_number=2,
            provider=self.config.LLM_PROVIDER,
            raw_response=response,
            parsed_response=result,
        )

        logger.info(
            "æå–å®Œæˆ: %s ä¸ªæ—¶é—´çº¿èŠ‚ç‚¹, %s ä¸ªå½±å“ç±»åˆ«",
            len(result.get("timeline", [])),
            len(result.get("impact", {})),
        )

        return {
            "timeline": result.get("timeline", []),
            "impact": result.get("impact", {}),
        }

    def _extract_media_from_validation(
        self, validation_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»æ­¥éª¤1çš„éªŒè¯ç»“æœä¸­æå–å¤šåª’ä½“å†…å®¹ï¼ˆä¸å†å•ç‹¬è°ƒç”¨LLMï¼‰ã€‚"""
        # ä»éªŒè¯åçš„ç›¸å…³é¡¹ä¸­ç­›é€‰å‡ºå¤šåª’ä½“å†…å®¹
        relevant_items = validation_result.get("relevant_items", [])
        
        # è¯†åˆ«å¤šåª’ä½“å†…å®¹ï¼ˆé€šè¿‡channelæˆ–typeå­—æ®µï¼‰
        media_items = []
        for item in relevant_items:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šåª’ä½“å†…å®¹
            channel = item.get("channel", "")
            item_type = item.get("type", "")
            # å¤šåª’ä½“æ¸ é“åŒ…æ‹¬: media, social, æˆ–è€…typeä¸ºmediaçš„
            if channel in {"media", "social"} or item_type == "media":
                media_items.append(item)
        
        # é™åˆ¶å¤šåª’ä½“æ•°é‡ï¼ˆæœ€å¤š10æ¡ï¼ŒæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        if len(media_items) > 10:
            # æŒ‰relevance_scoreæ’åºï¼Œå–å‰10æ¡
            media_items.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)
            media_items = media_items[:10]
            logger.info("å¤šåª’ä½“å†…å®¹è¿‡å¤šï¼Œé™åˆ¶ä¸ºå‰10æ¡ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰")
        
        logger.info("ä»éªŒè¯ç»“æœä¸­æå–äº† %s æ¡å¤šåª’ä½“å†…å®¹", len(media_items))
        
        return {
            "selected_items": media_items,
            "rejected_items": [],  # ä¸ç›¸å…³çš„å·²ç»åœ¨æ­¥éª¤1ä¸­è¢«æ’é™¤äº†
        }

    def _step4_report_generation(
        self,
        context: EventContext,
        event_info: Dict[str, Any],
        extraction_result: Dict[str, Any],
        validation_result: Dict[str, Any],
        media_result: Dict[str, Any],
    ) -> str:
        """æ­¥éª¤ 4: æŠ¥å‘Šç”Ÿæˆã€‚"""
        # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„æ–°é—»å’Œå¤šåª’ä½“æ¥æº
        all_sources = []
        
        # æ·»åŠ éªŒè¯åçš„æ–°é—»æ¥æºï¼ˆæ’é™¤åª’ä½“ï¼‰
        relevant_items = validation_result.get("relevant_items", [])
        for item in relevant_items:
            # åªæ·»åŠ éåª’ä½“å†…å®¹
            channel = item.get("channel", "")
            if channel not in {"media", "social"} and item.get("url"):
                all_sources.append({
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "summary": item.get("summary", ""),
                    "source": item.get("source", ""),
                    "published_at": item.get("published_at", ""),
                    "type": "news",
                })
        
        # æ·»åŠ ç­›é€‰åçš„å¤šåª’ä½“æ¥æºï¼ˆä»æ­¥éª¤3æå–çš„ï¼‰
        selected_media = media_result.get("selected_items", [])
        for item in selected_media:
            if item.get("url"):
                all_sources.append({
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "summary": item.get("summary", ""),
                    "source": item.get("source", ""),
                    "published_at": item.get("published_at", ""),
                    "type": "media",
                })
        
        # æ„å»º prompt
        messages = build_report_prompt(
            event_info=event_info,
            timeline=extraction_result.get("timeline", []),
            impact=extraction_result.get("impact", {}),
            media=all_sources,  # ä¼ é€’æ‰€æœ‰å¯ç”¨çš„æ¥æºï¼ˆæ–°é—»+å¤šåª’ä½“ï¼‰
            verified_facts=[],  # å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨
            conflicts=[],  # å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨
        )
        
        # è®°å½• LLM è¯·æ±‚
        from ..utils.detailed_logger import get_detailed_logger
        detailed_logger = get_detailed_logger()
        detailed_logger.log_llm_request(
            step="æŠ¥å‘Šç”Ÿæˆ",
            step_number=4,
            provider=self.config.LLM_PROVIDER,
            model=self.config.OPENAI_MODEL if self.config.LLM_PROVIDER == "openai" else self.config.GEMINI_MODEL,
            prompt_messages=messages,
            config={
                "temperature": self.config.LLM_TEMPERATURE,
                "max_tokens": self.config.LLM_MAX_TOKENS,
            },
        )

        # è°ƒç”¨ LLM
        client = self._get_client()
        response = client.chat(
            messages=messages,
            temperature=self.config.LLM_TEMPERATURE,
            max_tokens=self.config.LLM_MAX_TOKENS,
        )

        # è®°å½• LLM å“åº”ï¼ˆæŠ¥å‘Šæ˜¯çº¯æ–‡æœ¬ï¼Œä¸æ˜¯JSONï¼‰
        detailed_logger.log_llm_response(
            step="æŠ¥å‘Šç”Ÿæˆ",
            step_number=4,
            provider=self.config.LLM_PROVIDER,
            raw_response=response,
            parsed_response=None,  # æŠ¥å‘Šä¸æ˜¯JSONæ ¼å¼
        )

        return response

